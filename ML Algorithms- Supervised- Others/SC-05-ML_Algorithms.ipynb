{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "source": [
    "![](http://thecads.org/wp-content/uploads/2017/02/adax_logo.jpg)\n",
    "# Module 4: Machine Learning Algorithms\n",
    "\n",
    "In this module, we look at an array of machine learning algorithms, or techniques (for both regression and classification) that are commonly used by data scientists to perform predictive analytics.\n",
    "1. k-Nearest Neighbor (kNN)\n",
    "2. Softmax Regression\n",
    "3. Naive Bayes\n",
    "4. Support Vector Machine (SVM)\n",
    "5. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. k-Nearest Neighbor (kNN)\n",
    "\n",
    "The k-NN algorithm is arguably the simplest machine learning algorithm. Building the model consists only of storing the training dataset. To make a prediction for a new data point, the algorithm simply finds the closest data point(s) in the training dataset, basically its *nearest neighbors*. What we do after that, depends whether we are performing a classification or regression task.\n",
    "\n",
    "### kNN Classification\n",
    "\n",
    "In its simplest version, the kNN algorithm only considers exactly one nearest neighbor, which is the closest training data point to the point we want to make a prediction for. The prediction is then simply the known output for this training point. In other words, the class of the label is predicted as the class of its closest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n",
    "\n",
    "# build the model\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "training_accuracy = clf.score(X_train, y_train)   # record training set accuracy  \n",
    "test_accuracy = clf.score(X_test, y_test)         # record generalization accuracy\n",
    "\n",
    "print(\"Training Accuracy: \", training_accuracy)\n",
    "print(\"Test Accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the difference in training and test accuracy (or sometimes known as *generalization accuracy*) indicate?\n",
    "\n",
    "Instead of considering only the closest neighbor, we can also consider an arbitrary number, *k* of neighbors. This is where the *k* in the name *k*-nearest neighbor comes from. When considering more than one neighbor, a \"voting\" strategy is used to assign the label. That means, for each test point, we count how many neighbors belong to each class. Then, we assign it to the class that occured most frequently among its neighbors; in other words, the *majority class*. In cases where a tie is needed to be broken, the typical remedy is to decrease *k* (gradually) to arrive at a matched class.\n",
    "\n",
    "Let's have a look at the correlation between the first feature (mean radius) and all other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.c_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', 40)\n",
    "\n",
    "df = pd.DataFrame(np.c_[cancer['data'], cancer['target']],\n",
    "                  columns= np.append(cancer['feature_names'], ['target']))\n",
    "\n",
    "# show the whole correlation heatmap\n",
    "ax = sns.heatmap(df.corr())\n",
    "plt.show()\n",
    "\n",
    "dfc = df.corr()\n",
    "dfc[0:2]  # look at the first two row: correlation between mean radius and other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick two good features to perform classification (no guarantee it will be better than using all 30 features. Just for exercise!).\n",
    "\n",
    "\"**Mean concave points**\" feature is highly correlated with \"**mean radius**\". Let's choose these two as features.\n",
    "\n",
    "To plot the scatter plot of these two features, we can either opt for the *matplotlib* way or the *seaborn* way. We shall have the opportunity to try them both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### matplotlib way\n",
    "\n",
    "# specify training data as arrays\n",
    "#X1 = X_train[:,0]  # mean radius\n",
    "#X2 = X_train[:,7]  # mean concave points\n",
    "#y = y_train\n",
    "#classes = list(set(y))\n",
    "#fig, ax = plt.subplots()\n",
    "#sc = ax.scatter(X1, X2, c=y, cmap='viridis', edgecolors='grey', label=classes)\n",
    "\n",
    "### seaborn way\n",
    "\n",
    "# put training data back into dataframe\n",
    "df_train = pd.DataFrame(X_train, columns=cancer['feature_names'])\n",
    "df_train['target'] = pd.Series(y_train)\n",
    "\n",
    "grid = sns.pairplot(x_vars=\"mean radius\", y_vars=\"mean concave points\", data=df_train, hue=\"target\", palette=\"hls\", \n",
    "                    size=5, aspect=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### matplotlib way\n",
    "\n",
    "# specify training data as arrays\n",
    "X1 = X_train[:,0]  # mean radius\n",
    "X2 = X_train[:,7]  # mean concave points\n",
    "y = y_train\n",
    "classes = list(set(y))\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(X1, X2, c=y, cmap='viridis', edgecolors='grey', label=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some separation between data points from both classes. However, it gets a bit muddled at the center, with both classes' distributions overlapping slightly. Most model-based classification methods attempt to find a finite set of parameters or coefficients, hence they are also known as *parametric* methods. KNN is a non-parametric method because we don't have an objective function that we aim at minimizing or maximizing, hence it relies on some algorithmic rules to perform classification. \n",
    "\n",
    "Let's get a sample test data point and see its predicted label, and where it lies in the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just use the first point (index 0) from the test set\n",
    "newpoint = X_test[0,:].reshape(1, -1)  # recall: reshape needed to extend a 1D array to a 2D array of 1xN features\n",
    "print(\"Actual label: \", y_test[0])\n",
    "print(\"Predicted label: \", int(clf.predict(newpoint)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_test[0,:])\n",
    "print()\n",
    "print(newpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### matplotlib way\n",
    "#X1_new = X_test[0,0]\n",
    "#X2_new = X_test[0,7]\n",
    "#ax.scatter(X1_new, X2_new, c='r')\n",
    "#fig\n",
    "\n",
    "### seaborn way -- this is a bit more complex\n",
    "\n",
    "# get the single test new data sample, construct df, concatenate with training df\n",
    "X_one = X_test[0,:].reshape(1, -1)\n",
    "y_one = y_test[0]\n",
    "df_test = pd.DataFrame(X_one, columns=cancer['feature_names'])       # dataframe of a single row of data!\n",
    "df_test['target'] = ['new data']                                     # give it an unknown label like 'new data' \n",
    "full_ds = pd.concat([df_train, df_test])                             # concat that single row to the training df\n",
    "#print(full_ds)                                                       # inspect the df's last row...\n",
    "\n",
    "grid = sns.pairplot(x_vars=\"mean radius\", y_vars=\"mean concave points\", data=full_ds, hue=\"target\", hue_order=[0, 1, \"new data\"],\n",
    "                    size=5, aspect=1.5)\n",
    "\n",
    "# inspect closer -- who are the neighbors??\n",
    "#plt.xlim(12, 14)\n",
    "#plt.ylim(0.018, 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = sns.pairplot(x_vars=\"mean radius\", y_vars=\"mean concave points\", data=full_ds, hue=\"target\", hue_order=[0, 1, \"new data\"],\n",
    "                    size=5, aspect=1.5)\n",
    "\n",
    "plt.xlim(12, 14)\n",
    "plt.ylim(0.018, 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have used earlier are all 30 features. Let's revisit the 2 features we chose earlier for scatter analyis and use them as the only features for kNN classification. This way, we can try to visualize the \"decision boundary\" that separates the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-train classifier using only these two features. Use the same train-test split\n",
    "\n",
    "# re-form data to only have the 2 features (mean radius at index 0, mean concave points at index 7)\n",
    "X_train2 = X_train[:,[0, 7]]\n",
    "X_test2 = X_test[:,[0, 7]]\n",
    "y = y_train\n",
    "\n",
    "# build another model                   \n",
    "clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf2.fit(X_train2, y_train)\n",
    "\n",
    "# record the accuracy\n",
    "training_accuracy = clf2.score(X_train2, y_train)\n",
    "test_accuracy = clf2.score(X_test2, y_test)\n",
    "    \n",
    "print(\"Training Accuracy: \", training_accuracy)\n",
    "print(\"Test Accuracy: \", test_accuracy)\n",
    "\n",
    "classes = list(set(y))\n",
    "fig, axes = plt.subplots()\n",
    "sc = axes.scatter(X_train2[:,0], X_train2[:,1], c=y, cmap='jet', edgecolors='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function `plot_2d_classification` that takes a classifier and plots the decision boundary that has been learned from training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_2d_classification(classifier, X, ax=None, eps=None, alpha=1, cm='jet'):\n",
    "    \n",
    "    # get eps values automatically from distribution of feature values\n",
    "    if eps is None:\n",
    "        eps0 = X[:,0].std() / 2\n",
    "        eps1 = X[:,1].std() / 2\n",
    "    else:\n",
    "        eps0, eps1 = eps\n",
    "\n",
    "    # if no axes defined, get current axes\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - eps0, X[:, 0].max() + eps0\n",
    "    y_min, y_max = X[:, 1].min() - eps1, X[:, 1].max() + eps1\n",
    "    xx = np.linspace(x_min, x_max, 1000)\n",
    "    yy = np.linspace(y_min, y_max, 1000)\n",
    "\n",
    "    X1, X2 = np.meshgrid(xx, yy)\n",
    "    X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
    "    decision_values = classifier.predict(X_grid)\n",
    "    \n",
    "    ax.imshow(decision_values.reshape(X1.shape), extent=(x_min, x_max,\n",
    "                                                         y_min, y_max),\n",
    "              aspect='auto', origin='lower', alpha=alpha, cmap=cm)\n",
    "    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plot_2d_classification(clf2, X_train2, ax=axes, alpha=.4)           # plots on top of the previous axes containing points\n",
    "axes.set_xlabel(\"feature 0\")\n",
    "axes.set_ylabel(\"feature 7\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normalization is a common pre-processing method to ensure that all data are lie on a similar scale, or can be normalized to produce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-do everything, this time normalizing data before splitting data and classification\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "\n",
    "# Important: normalize data - many ways of doing this\n",
    "  \n",
    "data_norm = StandardScaler().fit_transform(cancer.data) # Standardization (or z-score norm.) \n",
    "                                                        # remove mean, scale by std dev\n",
    "#data_norm = MinMaxScaler().fit_transform(cancer.data)  # Minmax scaling. Scale to range [min max]\n",
    "#data_norm = normalize(cancer.data, axis=1)              # Normalization. Scaling individual samples to have unit norm\n",
    "            # normalize data -- axis 1 is the columns, normalize each sample data\n",
    "            # normalize features -- axis 0 is the rows, normalize each feature\n",
    "\n",
    "# train test split\n",
    "X_train_norm, X_test_norm, y_train, y_test = train_test_split(data_norm, \n",
    "                                                              cancer.target, \n",
    "                                                              stratify=cancer.target, \n",
    "                                                              random_state=66)\n",
    "\n",
    "# re-form data to only have the 2 features\n",
    "X_train2n = X_train_norm[:,[0, 7]]\n",
    "X_test2n = X_test_norm[:,[0, 7]]\n",
    "\n",
    "# build another model                   \n",
    "clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf2.fit(X_train2n, y_train)\n",
    "\n",
    "# record the accuracy\n",
    "training_accuracy = clf2.score(X_train2n, y_train)\n",
    "test_accuracy = clf2.score(X_test2n, y_test)\n",
    "    \n",
    "print(\"Training Accuracy: \", training_accuracy)\n",
    "print(\"Test Accuracy: \", test_accuracy)\n",
    "\n",
    "#classes = list(set(y))\n",
    "fig, axes = plt.subplots()\n",
    "sc = axes.scatter(X_train2n[:,0], X_train2n[:,1], c=y_train, cmap='jet', edgecolors='grey')\n",
    "\n",
    "plot_2d_classification(clf2, X_train2n, ax=axes, alpha=.4)\n",
    "axes.set_xlabel(\"feature 0\")\n",
    "axes.set_ylabel(\"feature 7\")\n",
    "\n",
    "# give some ticks to show the range where the data resides\n",
    "axes.set_xticks((X_train2n[:,0].min(), X_train2n[:,0].max()))\n",
    "axes.set_yticks((X_train2n[:,1].min(), X_train2n[:,1].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-do everything, this time normalizing data before splitting data and classification\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "\n",
    "# Important: normalize data - many ways of doing this\n",
    "  \n",
    "#data_norm = StandardScaler().fit_transform(cancer.data) # Standardization (or z-score norm.) remove mean, scale by std dev\n",
    "data_norm1 = MinMaxScaler().fit_transform(cancer.data)  # Minmax scaling. Scale to range [min max]\n",
    "#data_norm = normalize(cancer.data, axis=1)              # Normalization. Scaling individual samples to have unit norm\n",
    "            # normalize data -- axis 1 is the columns, normalize each sample data\n",
    "            # normalize features -- axis 0 is the rows, normalize each feature\n",
    "\n",
    "# train test split\n",
    "X_train_norm, X_test_norm, y_train, y_test = train_test_split(data_norm1, cancer.target, stratify=cancer.target, random_state=66)\n",
    "\n",
    "# re-form data to only have the 2 features\n",
    "X_train2n = X_train_norm[:,[0, 7]]\n",
    "X_test2n = X_test_norm[:,[0, 7]]\n",
    "\n",
    "# build another model                   \n",
    "clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf2.fit(X_train2n, y_train)\n",
    "\n",
    "# record the accuracy\n",
    "training_accuracy = clf2.score(X_train2n, y_train)\n",
    "test_accuracy = clf2.score(X_test2n, y_test)\n",
    "    \n",
    "print(\"Training Accuracy: \", training_accuracy)\n",
    "print(\"Test Accuracy: \", test_accuracy)\n",
    "\n",
    "#classes = list(set(y))\n",
    "fig, axes = plt.subplots()\n",
    "sc = axes.scatter(X_train2n[:,0], X_train2n[:,1], c=y_train, cmap='jet', edgecolors='grey')\n",
    "\n",
    "plot_2d_classification(clf2, X_train2n, ax=axes, alpha=.4)\n",
    "axes.set_xlabel(\"feature 0\")\n",
    "axes.set_ylabel(\"feature 7\")\n",
    "\n",
    "# give some ticks to show the range where the data resides\n",
    "axes.set_xticks((X_train2n[:,0].min(), X_train2n[:,0].max()))\n",
    "axes.set_yticks((X_train2n[:,1].min(), X_train2n[:,1].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-do everything, this time normalizing data before splitting data and classification\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "\n",
    "# Important: normalize data - many ways of doing this\n",
    "  \n",
    "#data_norm = StandardScaler().fit_transform(cancer.data) # Standardization (or z-score norm.) remove mean, scale by std dev\n",
    "#data_norm = MinMaxScaler().fit_transform(cancer.data)  # Minmax scaling. Scale to range [min max]\n",
    "data_norm2 = normalize(cancer.data, axis=1)              # Normalization. Scaling individual samples to have unit norm\n",
    "            # normalize data -- axis 1 is the columns, normalize each sample data\n",
    "            # normalize features -- axis 0 is the rows, normalize each feature\n",
    "\n",
    "# train test split\n",
    "X_train_norm, X_test_norm, y_train, y_test = train_test_split(data_norm2, \n",
    "                                                              cancer.target, \n",
    "                                                              stratify=cancer.target, \n",
    "                                                              random_state=66)\n",
    "\n",
    "# re-form data to only have the 2 features\n",
    "X_train2n = X_train_norm[:,[0, 7]]\n",
    "X_test2n = X_test_norm[:,[0, 7]]\n",
    "\n",
    "# build another model                   \n",
    "clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf2.fit(X_train2n, y_train)\n",
    "\n",
    "# record the accuracy\n",
    "training_accuracy = clf2.score(X_train2n, y_train)\n",
    "test_accuracy = clf2.score(X_test2n, y_test)\n",
    "    \n",
    "print(\"Training Accuracy: \", training_accuracy)\n",
    "print(\"Test Accuracy: \", test_accuracy)\n",
    "\n",
    "#classes = list(set(y))\n",
    "fig, axes = plt.subplots()\n",
    "sc = axes.scatter(X_train2n[:,0], X_train2n[:,1], c=y_train, cmap='jet', edgecolors='grey')\n",
    "\n",
    "plot_2d_classification(clf2, X_train2n, ax=axes, alpha=.4)\n",
    "axes.set_xlabel(\"feature 0\")\n",
    "axes.set_ylabel(\"feature 7\")\n",
    "\n",
    "# give some ticks to show the range where the data resides\n",
    "axes.set_xticks((X_train2n[:,0].min(), X_train2n[:,0].max()))\n",
    "axes.set_yticks((X_train2n[:,1].min(), X_train2n[:,1].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-do everything, this time normalizing data before splitting data and classification\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "\n",
    "# Important: normalize data - many ways of doing this\n",
    "  \n",
    "#data_norm = StandardScaler().fit_transform(cancer.data) # Standardization (or z-score norm.) remove mean, scale by std dev\n",
    "#data_norm = MinMaxScaler().fit_transform(cancer.data)  # Minmax scaling. Scale to range [min max]\n",
    "data_norm4 = normalize(cancer.data, axis=0)              # Normalization. Scaling individual samples to have unit norm\n",
    "            # normalize data -- axis 1 is the columns, normalize each sample data\n",
    "            # normalize features -- axis 0 is the rows, normalize each feature\n",
    "\n",
    "# train test split\n",
    "X_train_norm, X_test_norm, y_train, y_test = train_test_split(data_norm4, \n",
    "                                                              cancer.target, \n",
    "                                                              stratify=cancer.target, \n",
    "                                                              random_state=66)\n",
    "\n",
    "# re-form data to only have the 2 features\n",
    "X_train2n = X_train_norm[:,[0, 7]]\n",
    "X_test2n = X_test_norm[:,[0, 7]]\n",
    "\n",
    "# build another model                   \n",
    "clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf2.fit(X_train2n, y_train)\n",
    "\n",
    "# record the accuracy\n",
    "training_accuracy = clf2.score(X_train2n, y_train)\n",
    "test_accuracy = clf2.score(X_test2n, y_test)\n",
    "    \n",
    "print(\"Training Accuracy: \", training_accuracy)\n",
    "print(\"Test Accuracy: \", test_accuracy)\n",
    "\n",
    "#classes = list(set(y))\n",
    "fig, axes = plt.subplots()\n",
    "sc = axes.scatter(X_train2n[:,0], X_train2n[:,1], c=y_train, cmap='jet', edgecolors='grey')\n",
    "\n",
    "plot_2d_classification(clf2, X_train2n, ax=axes, alpha=.4)\n",
    "axes.set_xlabel(\"feature 0\")\n",
    "axes.set_ylabel(\"feature 7\")\n",
    "\n",
    "# give some ticks to show the range where the data resides\n",
    "axes.set_xticks((X_train2n[:,0].min(), X_train2n[:,0].max()))\n",
    "axes.set_yticks((X_train2n[:,1].min(), X_train2n[:,1].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did normalization of features helped improve the overall performance of the kNN classifier?\n",
    "\n",
    "**Note**: Understand the differences between the three types of normalization - Normalization by unit norm, Standardization (or z-score normalization) and Scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider using more neighbors in kNN algorithm. Will you get a \"smoother\" decision boundary? What is the impact of using more neighbors to the classification performance?\n",
    "\n",
    "**Q1**: What is the connection between model complexity (number of neighbors considered in computation) and the model's generalization capability (test classification performance)? *Hint: plot both training accuracy and test accuracy against number of neighbors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Regressor\n",
    "\n",
    "There is also a regression variant for the k-nearest neighbor algorithm. The prediction using a single neighbor is just the target value of the nearest neighbor. If k number of neighbors are used, the prediction is the average, or mean, of the neighbor values. Let's go back to the housing prices example, to try this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9c4f38c1c30c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhouse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"housing.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhouse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#house.corr()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "house = pd.read_csv(\"housing.csv\")\n",
    "house.head()\n",
    "\n",
    "#house.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house[house['total_rooms'].isnull() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = house.iloc[:, 7].values.reshape(-1, 1)        # feature: median income\n",
    "y = house.iloc[:, 8].values.reshape(-1, 1)        # target value: median_house_value\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "reg = KNeighborsRegressor(n_neighbors=3)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "sc = axes.scatter(X_train, y_train, cmap='jet', edgecolors='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Test set predictions:\\n{}\".format(reg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the model using the `score` method, which for regressors return the $R^2$ score. The $R^2$ score, also known as the *coefficient of determination*, is a measure of goodness of a prediction for a regression model, and yields a score between 0 and 1. A value of 1 corresponds to a perfect prediction, and a value of 0 corresponds to a constant model that just predicts the mean of the training set responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Training set R^2: {:.2f}\".format(reg.score(X_train, y_train)))\n",
    "print(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get somewhere around 0.3 for the test set. The model fitting is not that good actually. \n",
    "\n",
    "To visualize how the regressor fits to the data (remember, there's no \"model\" in this method), we create synthetic test dataset consisting of about 1,000 data points, evenly spaced between the min and max of the selected feature. This allows us to look in detail at how the regression fit looks like.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 1,000 data points, evenly spaced between the min and max of the feature selected\n",
    "minf = np.min(X) \n",
    "maxf = np.max(X)\n",
    "line = np.linspace(minf, maxf, 1000).reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# by increasing the number of neighbours\n",
    "\n",
    "minf = np.min(X) \n",
    "maxf = np.max(X)\n",
    "line = np.linspace(minf, maxf, 1000).reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps...\n",
    "* We can try with more neighbors?\n",
    "* We can try with more features (than just a single feature)?\n",
    "\n",
    "Experimentally, how would you figure these out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_acc_all2 = []\n",
    "test_acc_all2 = []\n",
    "K_max = 50\n",
    "for k in range(1,K_max):\n",
    "    reg2 = KNeighborsRegressor(n_neighbors=k)\n",
    "    reg2.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc_all2.append(reg2.score(X_train,y_train)) # the X_train2n data is normalized data. See previous codes\n",
    "    test_acc_all2.append(reg2.score(X_test,y_test))    # go for 75% split in train_test_split(test_size=0.75)\n",
    "                                                        # see what happens then\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(range(1,K_max), train_acc_all2, 'b-')\n",
    "plt.plot(range(1,K_max), test_acc_all2, 'r-')\n",
    "plt.xlabel(\"# of neighbours\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X2 = house.iloc[:, [3,7]].values.reshape(-1, 2)        # feature: median income\n",
    "y2 = house.iloc[:, 8].values.reshape(-1, 1)        # target value: median_house_value\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, random_state=0)\n",
    "\n",
    "train_acc_all2 = []\n",
    "test_acc_all2 = []\n",
    "K_max = 25\n",
    "for k in range(1,K_max):\n",
    "    reg2 = KNeighborsRegressor(n_neighbors=k)\n",
    "    reg2.fit(X_train2, y_train2)\n",
    "    \n",
    "    train_acc_all2.append(reg2.score(X_train2,y_train2)) # the X_train2n data is normalized data. See previous codes\n",
    "    test_acc_all2.append(reg2.score(X_test2,y_test2))    # go for 75% split in train_test_split(test_size=0.75)\n",
    "                                                        # see what happens then\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(range(1,K_max), train_acc_all2, 'b-')\n",
    "plt.plot(range(1,K_max), test_acc_all2, 'r-')\n",
    "plt.xlabel(\"# of neighbours\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X2.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two important parameters to the `KNeighborsRegressor` classifier: the number of neighbors and how you measure the distance between data points. Choosing the right distance measure is somewhat beyond the scope of our lesson; we can stick to Euclidean distance (with `p` set to 2), which works well in most cases. As for the number of neighbors, increasing this will result in \"smoother\" predictions, but it may not necessarily fit the training data well. On the other hand, using `n_neighbors=1` usually means predicted values will go through all of the data points -- too overly fitting.\n",
    "\n",
    "#### Strengths / Weakneses of the k-Nearest Neighbor algorithm\n",
    "\n",
    "One of the strengths of kNN is that the model is easy to understand, and gives a reasonable performance without too much adjustments or optimizations required. It serves as a good baseline method to consider before moving to more advanced techniques. \n",
    "\n",
    "An obvious weakness is in its speed. Its prediction can be quite slow if your training dataset is very large (either in number of samples or number of features). It also performs poorly on data with many features (hundreds or more) and data where most features are 0 most of the time (sparse datasets). Certainly, these drawbacks can be quite problematic in the real world, which makes kNN less than desirable.\n",
    "\n",
    "Watch this for some extra insight.\n",
    "https://www.youtube.com/watch?v=GbhZcvPLbQg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Revisiting Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())\n",
    "\n",
    "X = iris[\"data\"][:, 3:] # petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=23)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "print(log_reg)    # default: multi_class='ovr', C=1.0\n",
    "# basically ovr = one vs. the rest. If changed to \"multinomial\"\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict bunch of points within the range of 0 to 3 petal width, and get their predicted probabilities\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_new_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_new_proba[:, 0], 'b-')\n",
    "plt.plot(X_new, y_new_proba[:, 1], 'r-')\n",
    "plt.plot(X_new, y_new_proba[:, 2], 'g-')\n",
    "\n",
    "# Predict test data\n",
    "fig = plt.figure(figsize=(16,2))\n",
    "counter = range(0,len(y_test))\n",
    "y_test_proba = log_reg.predict_proba(X_test)\n",
    "plt.plot(counter, np.argmax(y_test_proba, axis=1), 'r^')   # red up arrows indicate predicted classes\n",
    "plt.plot(counter, y_test, 'bv')                            # blue down arrows indicate ground truth\n",
    "print(\"Accuracy: \",log_reg.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# this is a multi class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"data\"][:, 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "print(log_reg)    # default: multi_class='ovr', C=1.0\n",
    "# basically ovr = one vs. the rest. If changed to \"multinomial\"\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict bunch of points within the range of 0 to 3 petal width, and get their predicted probabilities\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_new_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_new_proba[:, 0], 'b-')\n",
    "plt.plot(X_new, y_new_proba[:, 1], 'r-')\n",
    "plt.plot(X_new, y_new_proba[:, 2], 'g-')\n",
    "\n",
    "# Predict test data\n",
    "fig = plt.figure(figsize=(16,2))\n",
    "counter = range(0,len(y_test))\n",
    "y_test_proba = log_reg.predict_proba(X_test)\n",
    "plt.plot(counter, np.argmax(y_test_proba, axis=1), 'r^')   # red up arrows indicate predicted classes\n",
    "plt.plot(counter, y_test, 'bv')                            # blue down arrows indicate ground truth\n",
    "print(\"Accuracy: \",log_reg.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# this is a multi class classification\n",
    "# each line represents the probability of classified into that class.\n",
    "# the sum of probability is 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the multi-class setting for LogisticRegression is 'ovr' or \"One-versus-Rest\", which basically returns the predicted probabilities of all three models (class 0 vs. the rest, class 1 vs. the rest, and class 2 vs. the rest). As you can see, the predicted probabilities of all three models near the middle range of petal widths (around 1.0) are a bit ambiguous. You can see a number of test samples that have been confused for class 2 (red arrow) instead of class 1 (blue arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression\n",
    "\n",
    "The Logistic Regression model can be generalized to support multiple classes directly, without having to train and combine multiple binary classifiers (such as KNNs and SVMs as we shall see later). This is called a *Softmax Regression* or *Multinomial Logistic Regression*.\n",
    "\n",
    "Given an instance $\\textbf{x}$, the Softmax Regression model first computes a score $s_k(\\textbf{x})$ for each class $k$, then estimates the probability of each class by applying the *softmax* function (or normalized exponential) to the scores. The equation to compute $s_k(\\textbf{x})$ is familiar to you...\n",
    "\\begin{align}\n",
    "    s_k(\\textbf{x}) = w_k^T\\cdot\\textbf{x}\n",
    "\\end{align}\n",
    "Note that each class has its own set of coefficients $w_k$. Once the score of every class is computed for the instance $\\textbf{x}$, we can estimate the probability $p$ that the instance belongs to class $k$ by running the scores through the softmax function:\n",
    "\\begin{align}\n",
    "    p_k = \\sigma(s(\\textbf{x}))_k = \\frac{\\text{exp}(s_k(\\textbf{x})}{\\sum_{j=1}^{K}\\text{exp}(s_j(\\textbf{x}))}\n",
    "\\end{align}\n",
    "where $K$ is the number of classes, $s(\\textbf{x})$ is the vector containing the scores of each class for the instance $\\textbf{x}$ and $\\sigma(s(\\textbf{x}))_k$ is the estimated probability that the instance $\\textbf{x}$ belongs to class $k$ given the scores of each class for that instance.\n",
    "\n",
    "To predict the class, the Softmax Regression model predicts the class with the highest estimated probability:\n",
    "\\begin{align}\n",
    "    y = \\arg \\max_k \\sigma(s(\\textbf{x}))_k = \\arg \\max_k s_k(\\textbf{x}) = \\arg \\max_k (w_k^T\\cdot \\textbf{x})\n",
    "\\end{align}\n",
    "The *argmax* operator returns the value of a variable that maximizes a function. In this equation, it returns the value of $k$ that maximizes the estimated probability $\\sigma(s(\\textbf{x}))_k$. \n",
    "\n",
    "We need to train the model to estimate a high probability for the target class and vice versa. Hence, the suitable cost function is called the *cross entropy*, which measures how well a set of estimat_ed class probabilities match the target classespenalizes a model when it estimates a low probability for a target class and vice versa.\n",
    "\\begin{align}\n",
    "    J(w) = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K y_k^{(i)} log (p_k^{(i)})\n",
    "\\end{align}\n",
    "And, the gradient of this cost function with regards to $w_k$:\n",
    "\\begin{align}\n",
    "    \\nabla_{w_k} J(w) = \\frac{1}{m}\\sum_{i=1}^m (p_k^{(i)}-y_k^{(i)})\\textbf{x}^{(i)}\n",
    "\\end{align}\n",
    "Following this, we can perform gradient descent to find the set of parameters $\\textbf{w}$ that minimizes the cost function.\n",
    "\n",
    "For some extra notes, refer to: https://www.youtube.com/watch?v=LLux1SW--oM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat the rest of the process as before. \n",
    "\n",
    "After which, you can also repeat with more features, using both petal length and petal width:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use back the 2D plot to see how softmax regression finds the decision boundaries between the three iris classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]\n",
    "y = iris['target']\n",
    "\n",
    "## Write your codes here for softmax regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using the standard logistic regression\n",
    "\n",
    "## Write your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes classifiers are a family of classifiers that are quite similar to the linear models discussed earlier and in previous modules. However, they tend to be even faster in training. The price paid for this efficiency is that naive Bayes models often provide generalization performance that is slightly worse than that of linear classifiers like\n",
    "`LogisticRegression` and `LinearSVC`. The reason that Naive Bayes models are so efficient is that they learn parameters by\n",
    "looking at each feature individually and collect simple per-class statistics from each feature.\n",
    "\n",
    "There are three kinds of naive Bayes classifiers implemented in scikit-learn: `GaussianNB`, `BernoulliNB`, and `MultinomialNB`. `GaussianNB` can the most universal, and can be applied to any continuous data, while `BernoulliNB` assumes binary data and `MultinomialNB` assumes count data (that is, that each feature represents an integer count of something, like how often a word appears in a sentence). `BernoulliNB` and `MultinomialNB` are mostly used in text data classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=23)\n",
    "\n",
    "clf_nb = GaussianNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "\n",
    "print(clf_nb.predict(X_test))\n",
    "\n",
    "print(\"Train accuracy: \", clf_nb.score(X_train, y_train))\n",
    "print(\"Train accuracy: \", clf_nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat again with only two features, the same two we identified earlier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2 = X_train[:, [2, 3]]\n",
    "X_test2 = X_test[:, [2, 3]]\n",
    "\n",
    "clf_nb2 = GaussianNB()\n",
    "clf_nb2.fit(X_train2, y_train)\n",
    "\n",
    "print(\"Train accuracy: \", clf_nb2.score(X_train2, y_train))\n",
    "print(\"Train accuracy: \", clf_nb2.score(X_test2, y_test))\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "sc = axes.scatter(X_train2[:,0], X_train2[:,1], c=y_train, cmap='jet', edgecolors='grey')\n",
    "\n",
    "plot_2d_classification(clf_nb2, X_train2, ax=axes, alpha=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machine (SVM) is a very powerful and versatile machine learning algorithm, capable of\n",
    "performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most\n",
    "popular models in Machine Learning, and anyone interested in Machine Learning should have it in their\n",
    "toolbox. SVMs are particularly well suited for classification of complex but small- or medium-sized\n",
    "datasets. SVMs may struggle computationally with large datasets, but there are fast solvers to help with that as well.\n",
    "\n",
    "### Linear SVM Classification\n",
    "\n",
    "The fundamental idea behind SVMs is best explained with some pictures.\n",
    "\n",
    "![](resources/large-margin-classification.png)\n",
    "\n",
    "The left plot shows the decision boundaries of three possible linear classifiers. The model whose decision boundary is represented by the dashed line is so bad that it fails to separate the classes properly. The other two models work perfectly on this training set, but their decision boundaries come so close to the instances that these models will probably\n",
    "not perform as well on new instances. In contrast, the solid line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the two classes but also stays as far away from the closest training instances as possible. You can think of an SVM classifier as fitting the widest possible \"street\" (represented by the parallel dashed lines) between the classes. This is called large margin classification. Think of it as having a \"safety buffer\" between the two classes just in case we encounter new data that is more ambiguous. The two instances that located at the edge of the \"street\" are called the *support vectors*.\n",
    "\n",
    "The linear SVM classifier model predicts the class of a new instance $x$ by computing the decision function $w^T \\cdot x + b = w_1 x_1 + \\cdots + w_n x_n + b$. If the result is positive, the new predicted class $\\hat{y}$ is the positive class (1), else it is the negative class (0).\n",
    "\\begin{align}\n",
    "    \\hat{y} =\n",
    "    \\begin{cases} \n",
    "      0 & \\text{if } w^T \\cdot x + b < 0, \\\\\n",
    "      1 & \\text{if } w^T \\cdot x + b \\ge 0 \n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "![](resources/svm-feature-scaling.png)\n",
    "\n",
    "This example shows how important feature scaling is. Since the vertical scale is much larger than the horizontal scale, the widest possible street may be narrow along the larger scaled axis. The decision boundary is much more spaced out after feature scaling is performed (feature scaling can be done by `StandardScaler` in Scikit-Learn)\n",
    "\n",
    "#### Soft margin classification\n",
    "\n",
    "Imposing all instances to be off the street and on the right side, is called a *hard margin classification*. There's some issues with this: 1) Works only if data is linearly separable, 2) Sensitive to outliers. The left plot shows data which is linearly not separable, impossible! The right plot shows a margin that is very differently placed which accommodates the outliers but may not generalize well to new instances.\n",
    "\n",
    "![](resources/hard-margin-issues.png)\n",
    "\n",
    "What we need is something flexible, that allows us to find a good balance between keeping the street as wide as possible and limiting the margin violations (i.e. instances that end up in the middle of street). This is called *soft margin classification*.\n",
    "\n",
    "In SVM, we control this using the `C` hyperparameter: \n",
    "* Smaller `C` value leads to a wider street but more margin violations (think of: we take in some mistakes for the sake of better generalization later)\n",
    "* Larger `C` value leads to fewer margin violations but ends up with a smaller margin (think of: let's be strict and get the boundary more correct, but at the expense of a smaller room for error later)\n",
    "\n",
    "![](resources/hyperparameter-C.png)\n",
    "\n",
    "Tip: If your SVM model is overfitting the data too tightly, you can try \"regularizing\" it by reducing `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.964285714286\n",
      "Test accuracy:  0.947368421053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a116a6588>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEMLHSGIXQggfI4ldCCF8zP8Dbf2G4kaDVTcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1157ab00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # set the target class as Iris-Virginica\n",
    "\n",
    "svm_clf = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    "#(\"linear_svc\", SVC(C=1, kernel='linear')),\n",
    "(\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "))\n",
    "\n",
    "# as usual, do the splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=11)\n",
    "\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy: \",svm_clf.score(X_train, y_train))\n",
    "print(\"Test accuracy: \",svm_clf.score(X_test, y_test))\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X_train)    # manually scale it for purpose of plotting\n",
    "\n",
    "def plot_svc_decision_boundary(svm_clf, ax, xmin, xmax):\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    x0 = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
    "\n",
    "    margin = 1/w[1]\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "\n",
    "    # only runnable if using SVC and not LinearSVC\n",
    "    #svs = svm_clf.support_vectors_\n",
    "    #ax.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='white', edgecolors='grey')\n",
    "    \n",
    "    ax.plot(x0, decision_boundary, \"k-\", linewidth=2)\n",
    "    ax.plot(x0, gutter_up, \"k--\", linewidth=2)\n",
    "    ax.plot(x0, gutter_down, \"k--\", linewidth=2)\n",
    "  \n",
    "fig, axsvm = plt.subplots()\n",
    "clf = svm_clf.steps[1][1]\n",
    "plot_svc_decision_boundary(clf, axsvm, X_train[:,0].min(), X_train[:,0].max())\n",
    "axsvm.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # set the target class as Iris-Virginica\n",
    "\n",
    "svm_clf2 = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"linear_svc\", SVC(C=3, kernel='linear')),\n",
    "#(\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "))\n",
    "\n",
    "# as usual, do the splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=11)\n",
    "\n",
    "svm_clf2.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy: \",svm_clf2.score(X_train, y_train))\n",
    "print(\"Test accuracy: \",svm_clf2.score(X_test, y_test))\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X_train)    # manually scale it for purpose of plotting\n",
    "\n",
    "fig, axsvm = plt.subplots()\n",
    "clf = svm_clf2.steps[1][1]\n",
    "plot_svc_decision_boundary(clf, axsvm, X_train[:,0].min(), X_train[:,0].max())\n",
    "axsvm.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: You can also use the `SVC` class, by `SVC(kernel=\"linear\", C=1)`, but it is much slower, especially with large training sets, thus it is not recommended. Another option is to use `SGDClassifier` class, with `SGDClassifier(loss=\"hinge\", alpha=1/(m*C))`. This applies a normal Stochastic Gradient Descent to train a linear SVM classifier. It is slightly slower than the `LinearSVC`, but its plus point is that it can handle huge datasets very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVM Classification\n",
    "\n",
    "Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets\n",
    "are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more\n",
    "features, such as polynomial features. This means that a distribution of data in a lower dimension space can be better separated if \"viewed\" from a higher dimension space. How to make sense of this? \n",
    "\n",
    "Using higher-order features also mean that the decision boundaries will now be nonlinear in nature. This allows data that are previously not linearly separable to be separated.\n",
    "\n",
    "To see how this works, we need some kind of nonlinearly distributed sample data to test this out. Scikit-learn comes with some data generators such as `make_moons` that can generate a synthetic toy dataset consisting of two interleaving half circles or \"moons\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(noise=0.3, random_state=0)\n",
    "\n",
    "polynomial_svm_clf = Pipeline((\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=1, loss=\"hinge\"))\n",
    "    ))\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "print(polynomial_svm_clf.steps)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "fig, axm = plt.subplots()\n",
    "axm.scatter(X[:,0], X[:,1], c=y, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(noise=0.3, random_state=0)\n",
    "\n",
    "polynomial_svm_clf1 = Pipeline((\n",
    "#    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(C=5, kernel='poly', degree=15, coef0=1))\n",
    "    ))\n",
    "polynomial_svm_clf1.fit(X, y)\n",
    "print(polynomial_svm_clf1.steps)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "fig, axm = plt.subplots()\n",
    "axm.scatter(X[:,0], X[:,1], c=y, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can already see that it is going to be quite a mean task to separate the two classes! But obviousy, you can picture how the ideal decision boundary would look like...something that curves around the two groups of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_2d_classification(polynomial_svm_clf1, X, ax=axm, alpha=.4) \n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viola! A nice snake-looking boundary that nicely bends arounds the two moon shaped classes.\n",
    "\n",
    "**Q2**: Generate a few plots with different `C` hyperparameter values and different polynomial feature degrees, and observe how the boundary changes with these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have observed, the plotting of the decision boundaries take a long time when we use a high-degree polynomial features (if you recall, what happens when we plot the model boundary is that it performs prediction on a million data points on the grid). \n",
    "\n",
    "#### Kernel trick\n",
    "\n",
    "A useful miraculous technique that is commonly used in machine learning is called a **\"kernel trick\"**, which makes it possible to add many polynomial features (even high-degree polynomials), without actually having to add them. In this trick, no additional features are actually \"added\", instead it creates a \"mapping\" function, let's say $\\phi$ to apply onto the original feature, transforming the features to the new higher order values. Here's an example of a 2nd-degree polynomial mapping function $\\phi$ that you want to apply.\n",
    "\n",
    "\\begin{align}\n",
    "    \\phi(\\textbf{x}) = \\phi\n",
    "    \\begin{pmatrix} \\begin{pmatrix} \n",
    "        x_{1} \\\\ \n",
    "        x_{2} \\\\\n",
    "    \\end{pmatrix} \\end{pmatrix} = \n",
    "    \\begin{pmatrix} \n",
    "        x_{1}^2 \\\\ \n",
    "        \\sqrt 2 x_1 x_2 \\\\\n",
    "        x_{2}^2 \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Notice that the transformed vector is 3-dimensional instead of 2-dimensional. By computing the dot product of two transformed vectors, $\\textbf{a}$ and $\\textbf{b}$),\n",
    "\n",
    "\\begin{align}\n",
    "    \\phi(\\textbf{a})^T \\cdot \\phi(\\textbf{b}) = \\text{...(after some algebra)...} = (\\textbf{a}^T \\cdot \\textbf{b})^2\n",
    "\\end{align}\n",
    "\n",
    "Now, here's the key insight. By applying the transformation $\\phi$ to all training instances, part of the formulation of SVM (which contains this dot product term), can be simply replaced by this mapping equation. \n",
    "\n",
    "In other words, the SVM has been \"kernelized\" to take on a different form, which provides the similar effect as if we add on more features of the data. The result will be strictly the same as if you went through the trouble of actually transforming the training set (by adding features) then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick.\n",
    "\n",
    "The function $K(\\textbf{a,b}) = (\\textbf{a}^T \\cdot \\textbf{b})^2$ is called a 2nd-degree polynomial kernel. Here are some other commonly used kernels:\n",
    "\n",
    "* Linear: $K(\\textbf{a,b}) = \\textbf{a}^T \\cdot \\textbf{b}$\n",
    "* Polynomial: $K(\\textbf{a,b}) = (\\gamma \\textbf{a}^T \\cdot \\textbf{b} + r)^d$\n",
    "* Gaussian RBF: $K(\\textbf{a,b}) = exp(-\\gamma \\mathbin{\\|}\\textbf{a-b}\\mathbin{\\|}^2)$\n",
    "* Sigmoid: $K(\\textbf{a,b}) = tanh(\\gamma \\textbf{a}^T \\cdot \\textbf{b} + r)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline((\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))     \n",
    "))\n",
    "poly_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "fig, axk = plt.subplots()\n",
    "axk.scatter(X[:,0], X[:,1], c=y, cmap='jet')\n",
    "plot_2d_classification(poly_kernel_svm_clf, X, ax=axk, alpha=.4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out with higher degree polynomials. It should work really fast using the kernel trick!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian RBF Kernel\n",
    "\n",
    "Just like the polynomial features method, we could use other ways of varying the features. Another popular way is to make use of a similarity function such as a Gaussian Radial Basis Function (RBF) which maps values to a bell-shaped function\n",
    "\n",
    "\\begin{align}\n",
    "    \\phi(\\textbf{x},\\ell) = exp(-\\gamma \\| \\textbf{x}-\\ell \\|^2)\n",
    "\\end{align}\n",
    "\n",
    "$\\gamma$ here is an additional hyperparameter which controls the width of the Gaussian curve, i.e. higher values yield a narrower curve which means an irregular jagged decision boundary, while smaller values yield a wider curve, which would mean a smoother decision boundary. $\\ell$ refers to a landmark or \"center\" reference point in which the distance from other points are calculated, $(\\textbf{x}-\\ell)$, hence the term \"radial\" carries that meaning. One simplest approach is to create a landmark at the location of each and every instance in the dataset, hence producing $N$ number of features for $N$ number of instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf = Pipeline((\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=1, C=0.001))\n",
    "))\n",
    "rbf_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "fig, axrbf = plt.subplots()\n",
    "axrbf.scatter(X[:,0], X[:,1], c=y, cmap='jet')\n",
    "plot_2d_classification(rbf_kernel_svm_clf, X, ax=axrbf, alpha=.4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to access the SVC object, we have to dig deep into the Pipeline object...\n",
    "print(rbf_kernel_svm_clf.steps[1][1].coef_) # SUPPOSE TO GIVE YOU ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**: Why do you think that the model coefficients (via `.coef_`) are only available when using a linear kernel?\n",
    "\n",
    "**Q4**: What does the Gaussian RBF kernel approximate to when $\\gamma$ tends towards 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in above, different gamme γ values results in different boundaries. \n",
    "# So gamma = 0.00000001 will give straight line\n",
    "# gamma = 1 gives beach shoreline\n",
    "# gamma = 5 gives a weird apple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we choose the right kernel type?\n",
    "\n",
    "With so many kernels to choose from, how can you decide which one to use? As a rule of thumb, you should always try the linear\n",
    "kernel first (remember that `LinearSVC`is much faster than `SVC(kernel=\"linear\")`), especially if the training set is very *large* or if it has plenty of features. If the training set is not too large, you should try the Gaussian RBF kernel as well; it works well in most cases. Then if you have spare time and computing power, you can also experiment with finding the optimal hyperparameter values for some of these kernels using cross-validation and grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Regression\n",
    "\n",
    "The SVM algorithm is quite versatile: it not only supports linear and nonlinear classification, but it also supports linear and nonlinear regression. The trick here is to reverse its objective -- instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations (i.e. instances off the street). The width of the street is controlled by a hyperparameter $\\epsilon$, which is the \"margin\" of the regressor. The figure on the left shows an example with $\\epsilon=1.5$, while the one on the right is with $\\epsilon=0.5$. Data points marked with a circle are those that violate the margin (outside the margin).\n",
    "\n",
    "![](resources/svm-regressor.png)\n",
    "\n",
    "For linear SVM Regression, we can use `LinearSVR` class from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# load boston house prices data\n",
    "boston = load_boston()\n",
    "print(boston.data.shape)\n",
    "\n",
    "X = boston.data[:,5]   # we select feature number 6        \n",
    "y = boston.target       \n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train = X_train.reshape(-1,1)\n",
    "X_test = X_test.reshape(-1,1)\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=0, C=100)\n",
    "svm_reg.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "sc = axes.scatter(X_train, y_train, cmap='jet', edgecolors='grey')\n",
    "\n",
    "def plot_svm_regression(svm_reg, X, y, axes):\n",
    "    x1s = np.linspace(X.min(), X.max(), 100).reshape(100, 1)\n",
    "    y_pred = svm_reg.predict(x1s)\n",
    "    axes.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "    axes.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n",
    "    axes.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n",
    "    #axes.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')\n",
    "    axes.plot(X, y, \"bo\")\n",
    "    plt.xlabel(r\"$x$\", fontsize=18)\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    #plt.axis(axes)\n",
    "    \n",
    "plot_svm_regression(svm_reg, X_train, y_train, axes)\n",
    "\n",
    "print(\"Training set R^2: {:.2f}\".format(svm_reg.score(X_train, y_train)))\n",
    "print(\"Test set R^2: {:.2f}\".format(svm_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle nonlinear regression tasks, you can use a kernelized SVM model.\n",
    "\n",
    "The `SVR` class is the regression equivalent of the `SVC` class, and the `LinearSVR` class is the regression equivalent of the `LinearSVC` class. The `LinearSVR` class scales linearly with the size of the training set (just like the `LinearSVC` class), while the `SVR` class gets too slow when the training set grows large (just like the `SVC` class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.0)\n",
    "svm_poly_reg.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "sc = axes.scatter(X_train, y_train, cmap='jet', edgecolors='grey')\n",
    "plot_svm_regression(svm_poly_reg, X_train, y_train, axes)\n",
    "\n",
    "print(\"Training set R^2: {:.2f}\".format(svm_poly_reg.score(X_train, y_train)))\n",
    "print(\"Test set R^2: {:.2f}\".format(svm_poly_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Trees\n",
    "\n",
    "Decision trees are widely used models for classification and regressions. Essentially, they learn a hierarchy of if/else questions, leading to an action.  \n",
    "\n",
    "These questions are similar to the questions you might ask in a game of 20 Questions. Imagine you want to distinguish between the following four animals: bears, hawks, penguins, and dolphins. Your goal is to get to the right answer by asking as few if/else questions as possible. You might start off by asking whether the animal has feathers, a question that narrows down your possible animals to just two. If the answer is \"yes\", you can ask another question that could help you distinguish between hawks and penguins. For example, you could ask whether the animal can fly. If the animal doesn’t have feathers, your possible animal choices are dolphins and bears, and you will need to ask a question to distinguish between these two animals—for example,\n",
    "asking whether the animal has fins. This series of questions can be expressed as a decision tree.\n",
    "\n",
    "Machine learning speaking, we want to build a model to distinguish between these four classes of animals using three features (has feathers, can fly, has fins). But instead of building these models by hand, we can learn them from data using supervised learning.\n",
    "\n",
    "### Building decision trees\n",
    "\n",
    "Usually, data does not come in the form of binary yes/no features, but is instead represented by continuous features. So, we expect the \"questions\" to be in \"threshold\"-like form, such as \"Is feature *i* larger than value *a*?\" To build a tree, the algorithm has to search over all possible conditions and finds the one that is most informative about the target variable. Using the two moons toy dataset, let's see how it's done.\n",
    "\n",
    "![](resources/decisiontrees-step1.png)\n",
    "\n",
    "First, splitting the dataset vertically at x[1]=0.0596 yields the most information; it best separates the points in class 1 from the points in class 2. The top node, also called the root, represents the whole dataset, consisting of 75 points belonging to class 0 and 75 points belonging to class 1. The split is done by testing whether x[1] <= 0.0596, indicated by a black line. If the test is true, a point is assigned to the left node, which contains 2 points belonging to class 0 and 32 points belonging to class 1. Otherwise the point is assigned to the right node, which contains 48 points belonging to class 0 and 18 points belonging to class 1. Even though the first split did a good job of separating the two classes, the bottom region still contains points belonging to class 0, and the top region still contains points belonging to class 1.\n",
    "\n",
    "![](resources/decisiontrees-step2.png)\n",
    "\n",
    "Next, we repeat the process of looking for the best (or the most informative) way to split further in both regions. This is depth 2 of the decision tree. This recursive process yields a binary tree of decisions, with each node containing a\n",
    "test condition. We can view this algorithm as building a hierarchical partition. The recursive partitioning of the data is repeated until each region in the partition (each leaf in the decision tree) only contains a single target value (a single class or a single regression value). A leaf of the tree that contains data points that all share the same target value is called *pure*. The final partitioning for this dataset is as follows:\n",
    "\n",
    "![](resources/decisiontrees-finalstep.png)\n",
    "\n",
    "#### Predicting the target class or value of new data point\n",
    "\n",
    "A prediction on a new data point is made by checking which region of the partition of the feature space the point lies in, and then predicting the target class with the majority (or the single target in the case of *pure* leaves) in that region. The region can be found by traversing the tree from the root and going left or right depending on whether the test condition is fulfilled or not.\n",
    "\n",
    "It is also possible to use decision trees for regression tasks, using exactly the same technique. To make a prediction, we traverse the tree based on the tests in each node and find the leaf the new data point falls into. The output for this data point is the mean target value of the training points in this leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the accuracy on the training set is 100%—because the leaves are pure, the tree was grown deep enough that it could perfectly memorize all the labels on the training data. Test set accuracy is slightly worse than the best result we can get from the linear models.\n",
    "\n",
    "If we don’t restrict the depth of a decision tree, the tree can become arbitrarily deep and complex. Unpruned trees are therefore prone to overfitting and not generalizing well to new data. Now let’s apply pre-pruning to the tree, which will stop developing the tree before the fitting becomes too accurate. Setting `max_depth` to a small number like 4 would mean the tree stops building after a certain depth has been reached. This decreases over-fitting, and can potentially produce better performance in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**: Find out what's the ideal depth level that we can strike a balance between fitting well to the training data, and generalizing well to test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing decision trees\n",
    "\n",
    "We can visualize the tree using the `export_graphviz` function from the tree module. This writes a file in the .dot file format, which is a text file format for storing graphs. We set an option to color the nodes to reflect the majority class in each node and pass the class and features names so the tree can be properly labeled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\n",
    "feature_names=cancer.feature_names, impurity=False, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can read this file and visualize it using the `graphviz` module on IPython/Jupyter, we first need to install a few new things:\n",
    "* Download GraphViz software from (http://graphviz.org/) and install it. After that, add the `%GRAPHVIZ_HOME_DIR%/bin/` folder to your `PATH` environment variable.\n",
    "* Open Anaconda prompt and install graphviz package: `pip install graphviz` (Restarting IPython/Jupyter may be necessary in most cases to have GraphViz activated thru the path)\n",
    "\n",
    "Alternatively, you can also use any program that can read `.dot` files to render it, but that will open up externally and not inside any IPython based environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conda install -c anaconda graphviz\n",
    "# pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "    d = graphviz.Source(dot_graph)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have this tree picture saved up externally, you can issue the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d.render('decision-tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method of inspecting the tree that may be helpful is to find out which path most of the data actually takes. The `samples` shown in each node gives the number of samples in that node, while `value` provides the number of samples per class. If you have noticed, the test conditions were chosen to create a very lop-sided distribution of samples from the two classes. Both left and right branches from the root node contained groups of samples that have one particular dominant class. Looking further down the tree, things may get more confusing. Instead of looking at the whole tree, which can be quite taxing, there are some useful properties that summarize the workings of the tree. The most commonly used summary is *feature importance*, which rates how important each feature is for the decision a tree makes. It is a number between 0 and 1 for each feature, where 0 means\"not used at all\" and 1 means \"perfectly predicts the target\". Feature importances always sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances_cancer(model):\n",
    "    n_features = cancer.data.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), cancer.feature_names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "\n",
    "plot_feature_importances_cancer(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the feature that was used in the top split (\"worst radius\") is by far the most important feature. This confirms our observation in analyzing the tree that the first level already separates the two classes fairly well. However, if a feature has a low feature importance, it does not mean that this feature is uninformative, it could mean that likely there may be another feature that encodes the same information. That feature was just simply not picked by the tree (in relative to other features). Feature importances tell us that \"worst radius\" is important, but not whether a high \"worst radius\" is indicative of a sample being benign or malignant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CART Training Algorithm\n",
    "\n",
    "How does this algorithm figure out how to construct a decision tree? One of the most commonly used method (and fast as well) is the Classification And Regression Tree (CART) algorithm which scikit-learn uses to grow these trees. The idea is quite simple: The algorithm splits the training set into two subsets using a single feature $k$ and a threshold $t_k$. How does it choose $k$ and $t_k$ It searches for the pair $(k, t_k)$ that produces the purest subsets (weighted by their size). The cost function that the algorithm tries to minimize is given by:\n",
    "\n",
    "\\begin{align}\n",
    "    J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}\n",
    "\\end{align}\n",
    "where $G_{left/right}$ measures the impurity of the left or right subset and $m_{left/right}$ is the number of instances in the left/right subset.\n",
    "\n",
    "Once it has successfully split the training set in two, it splits the subsets using the same logic, then the sub-sub-sets\n",
    "and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the `max_depth` hyperparameter), or if it cannot find a split that will reduce impurity.\n",
    "\n",
    "**Note**: Unfortunately, finding the optimal tree is known to be an \"NP-Complete\" problem: It requires $O(exp(m))$ time, making the problem intractable even for fairly small training sets. This is why we must settle for a \"reasonably good\" solution. Two measures are typically used to measure the impurity (either one): Gini impurity and Entropy. You may look up these terms to read more about it, but we are not covering this in the lesson.\n",
    "\n",
    "### Predicting classes\n",
    "\n",
    "To predict the class labels of the test instances, simply use `predict()` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree can also estimate the probability that an instance belongs to a particular class *k*: First it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class *k* in this node. Use `predict_proba`().\n",
    "\n",
    "### Performing regression\n",
    "\n",
    "Decision trees can also be used to perform regression tasks, via the implementation in `DecisionTreeRegressor`. The usage and analysis of regression trees is very similar to that of classification trees. There is one particular property of using tree-based models for regression that we want to point out, though. The `DecisionTreeRegressor` (and all other tree-based regression models) is not able to extrapolate, or make predictions outside of the range of the training data.\n",
    "\n",
    "Let's look at this example of a dataset consisting of historical computer memory (RAM) prices per unit Megabyte. Note the logarithmic scale of the y-axis. When plotting logarithmically, the relation seems to be quite linear and so it should be relatively easy to predict, apart from some bumps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ram_prices = pd.read_csv(\"ram_price.csv\")\n",
    "\n",
    "plt.semilogy(ram_prices.date, ram_prices.price)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price in $/Mbyte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a forecast for the years after 2000 using the historical data up to that point, with the date as our only feature. Then, we will use two simple models: a `DecisionTreeRegressor` and `LinearRegression`, for comparisons. The prices will all be rescaled using a logarithm, so that the relationship is relatively linear. After training the models and making predictions, we apply the exponential map to undo the logarithm transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# use historical data to forecast prices after the year 2000\n",
    "data_train = ram_prices[ram_prices.date < 2000]\n",
    "data_test = ram_prices[ram_prices.date >= 2000]\n",
    "\n",
    "# predict prices based on date\n",
    "X_train = data_train.date[:, np.newaxis]\n",
    "\n",
    "# we use a log-transform to get a simpler relationship of data to target\n",
    "y_train = np.log(data_train.price)\n",
    "tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# predict on all data\n",
    "X_all = ram_prices.date[:, np.newaxis]\n",
    "pred_tree = tree.predict(X_all)\n",
    "pred_lr = linear_reg.predict(X_all)\n",
    "\n",
    "# undo log-transform\n",
    "price_tree = np.exp(pred_tree)\n",
    "price_lr = np.exp(pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.semilogy(data_train.date, data_train.price, label=\"Training data\")\n",
    "plt.semilogy(data_test.date, data_test.price, label=\"Test data\")\n",
    "plt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\n",
    "plt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decision tree loses on the predictive capability compared to LinearRegression\n",
    "# decision tree cannot support new data. so it just predicts/estimates based on the last leaf it is at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear prediction approximates the data with a line, simply what we knew it would do. It gives a sufficiently good estimate (eyeball the actual test data which is the actual data). But, what exactly happened to the decision tree regressor?? Once it leaves the data range for which the model has data, the model simply keeps predicting the last known point. The tree has no ability to generate \"new\" responses, outside of what was seen during training data. This shortcoming applies to all models based on trees.\n",
    "\n",
    "**Q6**: Generate the visualization of the decision tree regressor. What would it show this time for `value` since we don't have specific class labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing them all\n",
    "![](resources/sample-comparisons.png)\n",
    "Here's how some of these classifiers that we have covered today, performed on various synthetic toy datasets. The first row using data generated by `make_moons()`, the second row using data generated by `make_circles()`, and the last row using `linearly separable` dataset from scikit-learn.\n",
    "\n",
    "**Q7**: Write a script (`.py` file) to automate the process of testing a number of machine learning algorithms on a particular dataset. You can create nice visualization plots (like those seen above), or you can compare their training and/or test accuracies using bar charts or plots. Do not forget to have proper data/feature normalization and training test splits. A good dataset to use is the breast cancer dataset.\n",
    "\n",
    "*Tip: To put multiple figures into one figure, use `subplots()`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercises\n",
    "\n",
    "**AE1**: Most of these classifiers are able to output probabilities during prediction. Write a function (or modify earlier plot functions) to provide a visualization of the class regions that uses these probabilities to show the \"confidence\" level of predicting the output across a range of input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AE2**: Kaggle Leaf Classification competition. Give it a try!\n",
    "\n",
    "Link: [https://www.kaggle.com/c/leaf-classification](https://www.kaggle.com/c/leaf-classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_metadata": {
   "author": "Andreas C. M\\\"ller",
   "title": "Machine Learning with Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
